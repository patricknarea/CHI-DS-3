{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to see what the underlying dimensions of a person's performance in the decathalon are.\n",
    "\n",
    "That is, people compete in 10 different sports in the decathalon, and receive scores for their performance in each.\n",
    "\n",
    "Are there a smaller set of dimensions that can explain the pattern of intercorrelation between the scores on different events in the decathalon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are first going to start by reading in the Wikipedia table with the decathalon scores.\n",
    "\n",
    "This is the same code we saw to read in HTML tables in the API scraping lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.html as ET\n",
    "\n",
    "def table_reader(source_code,table_number=0):\n",
    "    doc = ET.fromstring(source_code)\n",
    "    data=[]\n",
    "    rows = doc.xpath(\"//table\")[table_number].findall(\"tr\")  \n",
    "    for row in rows:\n",
    "        data.append([c.text_content() for c in row.getchildren()])\n",
    "    return data\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Athletics_at_the_2008_Summer_Olympics_%E2%80%93_Men%27s_decathlon\"\n",
    "content= requests.get(url)\n",
    "content_string = content.text.encode('utf-8')\n",
    "#content_string = content_string.replace(\"\\n\",\"\")\n",
    "data_table=table_reader(content_string,5)\n",
    "decathalon = pd.DataFrame(data_table[1:],\n",
    "                          index=range(len(data_table[1:])),\n",
    "                          columns=data_table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the event columns into a numeric format, so we are going to write a function that splits the current text on the exclamation point, and convert the text before it into a floating point number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_numeric(content):\n",
    "    try: return float(content.split(\"!\")[0])\n",
    "    except: return None\n",
    "    \n",
    "events = ['100 m',\"LJ\",'SP','HJ','400 m','110 m H','DT','PV','JT','1500 m']\n",
    "for event in events:\n",
    "    decathalon[event] = decathalon[event].apply(make_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's handle missing values by converting 0's into missing, and then drop the missing rows.\n",
    "\n",
    "Now we have a dataframe that has all of the scores participants received for each decthalon event.\n",
    "\n",
    "Higher scores indicate better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decathalon[decathalon==0]=np.nan\n",
    "decathalon = decathalon.dropna()\n",
    "print decathalon.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the correlation between those scores to see what performance on a certain event tends to correlate with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decathalon.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to import the principal component analysis function from sklearn and fit it to our data with differing numbers of components.\n",
    "\n",
    "We normalize (scale) all of our columns to have a mean of 0 and standard deviation of 1 so that the relative magnitude of the variables are all similar.\n",
    "\n",
    "We'll try components ranging from 1 to 10.\n",
    "\n",
    "When we fit the model, we'll see how much of the explained variance is accounted for by each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "pc = PCA(n_components=10)\n",
    "pc.fit(scale(decathalon[events]))\n",
    "pc.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these sum to 10 (as many components that we had).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(pc.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because each variance was scaled to have a variance of 1.0\n",
    "\n",
    "Each component explains some of the total variance.\n",
    "\n",
    "We can look at the ratio of explained variance, to see what percentage of the total variance (i.e., 10) is accounted for by each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Because we only want components that reduce the dimensionality, we will only take as many components that have an expained variance greater than 1.0\n",
    "\n",
    "If we took a component that had an explained variance less than 1, the component would explain variance less than the original variable did individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc = PCA(n_components=3)\n",
    "pc.fit(scale(decathalon[events]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the components_ function to see how each variable correlates to each component.\n",
    "\n",
    "These correlations between each of the variables and the components are called the \"factor loadng matrix\", and give you a sense of what each component represents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor_loading_matrix = pd.DataFrame(pc.components_.T,index=events)\n",
    "factor_loading_matrix.sort_values(by=[0,1,2],ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the correlations can be hard to interpret, so a technique called factor rotation, makes the factor correlations (loadings) more straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def varimax_rotation(Phi, gamma = 1.0, q = 80, tol = 1e-8):\n",
    "    from scipy import eye, asarray, dot, sum\n",
    "    from scipy.linalg import svd\n",
    "    p,k = Phi.shape\n",
    "    R = eye(k)\n",
    "    d=0\n",
    "    for i in xrange(q):\n",
    "        d_old = d\n",
    "        Lambda = dot(Phi, R)\n",
    "        u,s,vh = svd(dot(Phi.T,asarray(Lambda)**3 - (gamma/p) * dot(Lambda, np.diag(np.diag(dot(Lambda.T,Lambda))))))\n",
    "        R = dot(u,vh)\n",
    "        d = sum(s)\n",
    "        if d_old!=0 and d/d_old < 1 + tol: break\n",
    "    return dot(Phi, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rotated_factors = pd.DataFrame(varimax(factor_loading_matrix),index=events)\n",
    "rotated_factors.sort_values([0,1,2],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the large values in the rotation matrix for each column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Factor 1: Sprinting\n",
    "    \n",
    "#Factor 2: Arm Strength\n",
    "\n",
    "#Factor 3: Leg Strength\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can assign each person a facor score that describes how much that person's performance is related to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor_scores = fa.transform(scale(decathalon[events]))\n",
    "factor_scores_df = pd.DataFrame(factor_scores,columns=[\"Factor1\",\"Factor2\",\"Factor3\"])\n",
    "decathalon = pd.concat([decathalon,factor_scores_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some people who scored highest on factor 1 and what events they excelled in relative to people who scored low on factor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print decathalon.sort_values(by=\"Factor1\",ascending=False).head(5)\n",
    "print decathalon.sort_values(by=\"Factor1\",ascending=False).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because each person has a factor score for each of the 3 components, we can see how each score relates to overall performance in the decathalon, and how of much of a person's final performance can be predicted by these three components\n",
    "\n",
    "By examining the coefficient strength, we can see what attributes best predict performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def numeric(content):\n",
    "    return filter(lambda x: x in string.digits,content)\n",
    "overall_score = decathalon['Overall points'].apply(numeric)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(decathalon[[\"Factor1\",\"Factor2\",\"Factor3\"]],overall_score)\n",
    "print lr.score(decathalon[[\"Factor1\",\"Factor2\",\"Factor3\"]],overall_score)\n",
    "zip([\"Factor1\",\"Factor2\",\"Factor3\"],lr.coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis is similar to PCA in that it tries to reduce the dimensions of the variables.\n",
    "\n",
    "However, it is useful for finding the dimension that separates groups.\n",
    "\n",
    "For example, if you have two groups of people, and they each respond to different survey questions, you can try to see what is the common underlying factor that separates them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a survey about workplace values that were given to workers in East and West Germany.\n",
    "\n",
    "Every person was asked to rate the importance of 13 different workplace values.\n",
    "\n",
    "We will read in the two datasets, combine them together, and use the \"key\" function to keep track of which respondents came rom which area in Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = [\n",
    "\"Interesting job\",\n",
    "\"Independent work\",\n",
    "\"Much responsibility\",\n",
    "\"Meaningful job\",\n",
    "\"Chances for advancement\",\n",
    "\"Respected job\",\n",
    "\"Can help others\",\n",
    "\"Useful job\",\n",
    "\"Contact to other people\",\n",
    "\"Secure position\",\n",
    "\"High income\",\n",
    "\"Much spare time\",\n",
    "\"Healthy working condition\"\n",
    "]\n",
    "east_germany = pd.read_csv(\"work%20values%20east%20germany.dat.txt\",sep=\"\\t\",header=None,names=values)\n",
    "west_germany = pd.read_csv(\"work%20values%20west%20germany.dat.txt\",sep=\"\\t\",header=None,names=values)\n",
    "germany_values = pd.concat([east_germany,west_germany], keys=['East', 'West'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the ix function on a dataframe to access just the rows with the key \"East\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "germany_values.ix[\"East\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a variable that has the labels for where each respondent came from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location = [\"East\"]*len(germany_values.ix[\"East\"])\n",
    "location += [\"West\"]*len(germany_values.ix[\"West\"])\n",
    "print location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can provide our responses that we want to reduce into a dimmension, and their country to the LinearDiscriminant Analysis function.\n",
    "\n",
    "We'll scale the responses to have a mean of 0 and a standard deviation of 1 before processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "ld = LinearDiscriminantAnalysis()\n",
    "ld.fit(scale(germany_values),location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also assign each person a score on this dimension, with any value above 0 indicating that he/she belongs to class 1, and any values below 0 indicating that he/she belongs to class 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "germany_values['Factor'] = ld.transform(scale(germany_values))\n",
    "print germany_values['Factor'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows that any person with a factor prediction above 0 is predicted as being in \"West\" and any person with a factor prediction below 0 is predicted as being in \"East\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(ld.predict(scale(germany_values[values])),germany_values['Factor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the general accuracy of our discriminant dimension with the score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ld.score(scale(germany_values),location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the points along one dimension and see how many of the points scoring below 0 are indeed belonging to the appropriate class and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "vector = germany_values['Factor']\n",
    "n_east = len(germany_values.ix[\"East\"])\n",
    "\n",
    "n_west = len(germany_values.ix[\"West\"])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sn.regplot(vector[:n_west],np.array([0]*n_west),\n",
    "           fit_reg=False,\n",
    "           color=\"blue\")\n",
    "sn.regplot(vector[n_west:],np.array([0]*n_east),\n",
    "           fit_reg=False,\n",
    "           color=\"green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can see how each of the variables relate to the Discriminant component using the factor loading matrix (which is simply the correlation of the variables to the factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "germany_values.corr()['Factor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this compares to the group differences on each of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "germany_values.groupby(location).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "sn.clustermap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster analysis helps identify groups of variables that are similar to other variables in that group, but different from variables in other groups.\n",
    "\n",
    "It is usefull for understanding the different groupings of people and the natural grouping of variabes.\n",
    "\n",
    "The following dataset contains people rank ordered preferences (from 1 to 15) for different breakfast items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breakfast_items = [\"toast pop-up\",\n",
    "\"buttered toast\",\n",
    "\"English muffin and margarine\",\n",
    "\"jelly donut\",\n",
    "\"cinnamon toast\",\n",
    "\"blueberry muffin and margarine\",\n",
    "\"hard rolls and butter\",\n",
    "\"toast and marmalade\",\n",
    "\"buttered toast and jelly\",\n",
    "\"toast and margarine\",\n",
    "\"cinnamon bun\",\n",
    "\"Danish pastry\",\n",
    "\"glazed donut\",\n",
    "\"coffee cake\",\n",
    "\"corn muffin and butter\"]\n",
    "\n",
    "breakfast = pd.read_csv(\"breakfast%20items.dat.txt\",sep=\"\\t\",header=None,names=breakfast_items)\n",
    "breakfast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cluster both variables (breakfast items) and rows (people) using a heirarchical clustering method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sn.clustermap(breakfast.corr(),method=\"complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use a confirmatory approach to clustering where we cluster the columns by specifying the number of clusters ahead of time, and the cluster centers that minimize the distance between items in that cluster and maximize the distance between items in that cluster are found.\n",
    "\n",
    "We can ask the algorithm to try different numbers of clusters, and choose the cluster that gives the best explanation of variance amongst the items, before we start seeing diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "def elbow(df, n):\n",
    "    kMeansVar = [KMeans(n_clusters=k).fit(df.values) for k in range(1, n)]\n",
    "    centroids = [X.cluster_centers_ for X in kMeansVar]\n",
    "    k_euclid = [cdist(df.values, cent) for cent in centroids]\n",
    "    dist = [np.min(ke, axis=1) for ke in k_euclid]\n",
    "    wcss = [sum(d**2) for d in dist]\n",
    "    tss = sum(pdist(df.values)**2)/df.values.shape[0]\n",
    "    bss = tss - wcss\n",
    "    return bss / tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as P\n",
    "\n",
    "def _line_fit(x, y):\n",
    "    '''Return the slope m and intercept b of the best line fit y=mx+b to the data x[:k] and y[:k]\n",
    "    for k=0..len(y).''' \n",
    "    # Figure out the m and b (in the y=mx+b sense) for the \"left-of-knee\"\n",
    "    n, sigma_x, sigma_y, sigma_xx, sigma_xy = np.arange(1, len(y) + 1), np.cumsum(x), np.cumsum(y), np.cumsum(x * x), np.cumsum(x * y)\n",
    "    det = np.maximum(n * sigma_xx - sigma_x * sigma_x, 1e-15)\n",
    "    m = (n * sigma_xy - sigma_x * sigma_y) / det\n",
    "    b = -(sigma_x * sigma_xy - sigma_xx * sigma_y) / det\n",
    "    return m, b\n",
    "\n",
    "'''Reverse an array.'''\n",
    "_reverse = lambda x: x[-1::-1]\n",
    "\n",
    "\n",
    "def knee_pt(x, y, absolute_dev=True):   \n",
    "    \n",
    "    # Check input args\n",
    "    if len(y) < 3: raise ValueError('knee_pt: y must be at least 3 elements long')\n",
    "    # Sort x, y in increasing x-order\n",
    "    if any(np.diff(x) < 0):\n",
    "        idx = np.argsort(x)\n",
    "        x, y = x[idx], y[idx]\n",
    "    else: idx = np.arange(len(x))\n",
    "    \n",
    "    # Regression slope (m) and intercept (b) for the \"left-of-knee\"\n",
    "    m_fwd, b_fwd = _line_fit(x, y)\n",
    "    # Regression slope (m) and intercept (b) for the \"right-of-knee\"\n",
    "    m_bck, b_bck = map(_reverse, _line_fit(_reverse(x), _reverse(y)))\n",
    "                       \n",
    "    # Calculate sum of errors for left- and right- of-knee fits per point\n",
    "    error_curve = np.tile(np.inf, len(y))\n",
    "    for breakpt in xrange(1, len(y) - 1):\n",
    "        err_fwd = m_fwd[breakpt] * x[:breakpt] + b_fwd[breakpt] - y[:breakpt]\n",
    "        err_bck = m_bck[breakpt] * x[breakpt:] + b_bck[breakpt] - y[breakpt:]\n",
    "        error_curve[breakpt] = sum(abs(err_fwd)) + sum(abs(err_bck)) if absolute_dev else np.sqrt(sum(err_fwd * err_fwd)) + np.sqrt(sum(err_bck * err_bck))\n",
    "    # Find location of the min of the error_curve\n",
    "    return idx[np.argmin(error_curve)]\n",
    "\n",
    "\n",
    "def plot_knee_point(x, y, knee, text_offset=(180, -20)):\n",
    "    '''Generate a plot of the data y(x), and the knee point at knee (the index of that\n",
    "    point in the x- and y-arrays).'''\n",
    "    idx = np.argsort(x)\n",
    "    fig = P.figure()\n",
    "    P.clf()\n",
    "    ax = fig.add_subplot(111)\n",
    "    P.hold(True)\n",
    "    ax.plot(x[idx], y[idx], 'bx-', label='Data')\n",
    "    P.xlabel('x')\n",
    "    P.ylabel('y')\n",
    "    P.plot(x[knee], y[knee], 'ro', markersize=10)\n",
    "    P.annotate('Knee Point (%.3f,%.3f)' % (x[knee], y[knee]), xy=(x[knee], y[knee]), xytext=text_offset,\n",
    "               textcoords='offset points', ha='right', va='bottom',\n",
    "               bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "               arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    P.title('Curve and Knee')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the above code to plot the amount of variance explained for each cluster by the number of clusters.\n",
    "\n",
    "We want to find the knee, or where increasing the number of clusters leads to diminishing returns for the amount of variance explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "n_clusters = 10\n",
    "variances_explained = elbow(breakfast,n_clusters)\n",
    "factors = np.array(range(1,n_clusters))\n",
    "knee = knee_pt(factors, variances_explained)\n",
    "plot_knee_point(factors, variances_explained, knee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that 3 clusters are suggested, we can visualize their position by plotting them against different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3,precompute_distances=True)\n",
    "kmeans.fit(breakfast)\n",
    "kmeans.transform(breakfast)\n",
    "cluster_assignments = pd.Series(kmeans.predict(breakfast),name=\"Cluster\")\n",
    "combined = pd.concat([breakfast,cluster_assignments],axis=1)\n",
    "first_cluster = combined[(combined.Cluster == 0)]\n",
    "second_cluster = combined[(combined.Cluster == 1)]\n",
    "third_cluster = combined[(combined.Cluster == 2)]\n",
    "\n",
    "sn.regplot(first_cluster['buttered toast'],first_cluster['jelly donut'],color=\"red\",fit_reg=False)\n",
    "sn.regplot(second_cluster['buttered toast'],second_cluster['jelly donut'],color=\"blue\",fit_reg=False)\n",
    "scatter = sn.regplot(third_cluster['buttered toast'],third_cluster['jelly donut'],color=\"green\",fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the relative importance of each variable on the cluster by examining where the cluster is concentrated on that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_centers = pd.DataFrame([breakfast.columns,kmeans.cluster_centers_[0],\n",
    "              kmeans.cluster_centers_[1],\n",
    "              kmeans.cluster_centers_[2]\n",
    "              ]\n",
    "              ).T\n",
    "\n",
    "cluster_centers.sort_values(by=3,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import requests\n",
    "import lxml.etree as ET\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "url=\"http://www.infoplease.com/ipa/a0763098.html\"\n",
    "content= requests.get(url)\n",
    "content_string = content.text.encode('utf-8')\n",
    "content_string = content_string.replace(\"\\n\",\"\")\n",
    "data_table=table_reader(content_string,1)\n",
    "cities = pd.DataFrame(data_table,index=range(len(data_table)),columns=None)[1]\n",
    "cities = cities.str.replace(\"  \",\"\")\n",
    "\n",
    "pairs = itertools.combinations(cities[:15],2)\n",
    "\n",
    "i=1\n",
    "distances=[]\n",
    "for pair in pairs:\n",
    "    good=False\n",
    "    while good != True:\n",
    "        query=\"Distance from \"+pair[0]+\" to \"+pair[1]\n",
    "        content= requests.get(\"http://api.wolframalpha.com/v2/query?appid=JRUT3Q-J7X5RLLY5U&input=\"+query)    \n",
    "        content_string = content.text.encode(\"utf-8\")\n",
    "        try: \n",
    "            doc = ET.fromstring(str(content_string))\n",
    "            answer = doc.find('pod[@title=\"Result\"]/subpod/plaintext')\n",
    "        except:\n",
    "            print pair\n",
    "        \n",
    "        if answer != None:\n",
    "            result = answer.text.split(\" \")[0]\n",
    "            distances.append(result)\n",
    "            \n",
    "            good=True\n",
    "            answer=None\n",
    "            print i\n",
    "            i+=1\n",
    "        time.sleep(5)\n",
    "        \n",
    "from scipy.spatial.distance import squareform\n",
    "distance_matrix = squareform(distances)\n",
    "city_distances = pd.DataFrame(distance_matrix,index=cities[:15],columns=cities[:15])\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "city_distances = pd.read_csv(\"city_distances.txt\")\n",
    "city_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds = MDS(n_components = 2, dissimilarity=\"precomputed\" ,metric=False,n_init=1000,max_iter=1000)\n",
    "coordinates = mds.fit_transform(city_distances)\n",
    "print coordinates\n",
    "scatter = sn.regplot(coordinates[:,0], coordinates[:,1],fit_reg=False)\n",
    "ax = scatter.axes\n",
    "print mds.stress_\n",
    "for i in range(len(city_distances)):\n",
    "    ax.text(coordinates[i,0], coordinates[i,1], city_distances.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "supremecourt = pd.read_csv(\"supremecourtdistances.txt\",sep=\"\\t\")\n",
    "supremecourt = supremecourt.set_index(\"Judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "supremecourt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metric scaling uses the actual values of\n",
    "the dissimilarities, while nonmetric scaling effectively uses only their ranks (Shepard\n",
    "1962, Kruskal 1964a). Nonmetric MDS is realized by estimating an optimal monotone\n",
    "transformation f(Di,j ) of the dissimilarities simultaneously with the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "mds = MDS(n_components = 1, dissimilarity=\"precomputed\",metric=False,n_init=100,max_iter=100)\n",
    "mds.fit(supremecourt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stress metric is how we evaluate how well our scaling represented the distances. We want the stress value to be as close to 0 as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mds.stress_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the coordinates from our dimensions for each observation to see where on the dimension the observation lies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coordinates = mds.fit_transform(supremecourt)\n",
    "coordinates = coordinates.flatten()\n",
    "zip(supremecourt,coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these dimensions on a scatterplot.\n",
    "\n",
    "Because we are only using one dimension, we can set the coordinates for Y to always be 0, and treat our MDS scaling as the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import numpy as np\n",
    "% matplotlib inline\n",
    "\n",
    "y_coordinates = np.array([0]*len(supremecourt))\n",
    "scatter = sn.regplot(coordinates, y_coordinates,fit_reg=False)\n",
    "ax = scatter.axes\n",
    "ax.set_ylim(-.05,.05)\n",
    "for i in range(len(supremecourt)):\n",
    "    ax.text(coordinates[i], 0.025, supremecourt.columns[i],rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping people's perceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "face_header = [\n",
    "\"Grief at death of mother\",      \n",
    "\"Savoring a coke\",      \n",
    "\"Very pleasant surprise\",            \n",
    "\"Maternal love-baby in arms\",       \n",
    "\"Physical exhaustion\",               \n",
    "\"Something wrong with plane\",       \n",
    "\"Anger at seeing dog beaten\",        \n",
    "\"Pulling hard on seat of chair\",    \n",
    "\"Unexpectedly meets old boy friend\",\n",
    "\"Revulsion\",             \n",
    "\"Extreme pain\",                      \n",
    "\"Knows plane will crash\",         \n",
    "\"Light sleep\" \n",
    "]\n",
    "faces = pd.read_csv(\"facial%20expressions.dat.txt\",sep=\"\\t\",header=None, names=crime_header)\n",
    "faces.index = face_header \n",
    "\n",
    "faces_mds = MDS(n_components=2,dissimilarity=\"precomputed\",metric=False,n_init=100,max_iter=100)\n",
    "\n",
    "coordinates = faces_mds.fit_transform(faces)\n",
    "print faces_mds.stress_\n",
    "scatter = sn.regplot(coordinates[:,0], coordinates[:,1],fit_reg=False)\n",
    "ax = scatter.axes\n",
    "ax.autoscale_view()\n",
    "for i in range(len(crimes)):\n",
    "    ax.text(coordinates[i,0], coordinates[i,1], faces.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify what the dimensions mean by having each stimulus rated on the assumed interpretation and then correlating the ratings with the coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scales = pd.read_csv(\"facial%20expressions%20scales.dat.txt\",sep=\"\\t\",header=None,names=[\n",
    "        \"pleasant-unpleasant\", \n",
    "        \"attention-rejection\",\n",
    "        \"tension-sleep\"])\n",
    "print np.corrcoef(coordinates[:,1],scales['pleasant-unpleasant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.corrcoef(coordinates[:,0],scales['tension-sleep'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
